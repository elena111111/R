# Метрические алгоритмы классификации

Для таких алгоритмов должна выполняться гипотеза компактности: схожим объектам соответствуют схожие ответы.
Вводится мера близости объектов (в нашем случае евклидово расстояние), которая показывает, насколько эти объекты похожи. 

## knn - метод k ближайших соседей.
Дана выборка ирисов Фишера(150 элементов), в ней 3 класса(*setosa*, *versicolor*, *virginica*). 
Мы хотим классифицировать множество точек *{z}* по 2м признакам (Petal.Length, Petal.Width). Эти же признаки - координаты точек на графике.

Реализация:

```
knn <- function(z, X, k){	# X - это Ирисы Фишера, k - число соседей точки z
  xl <- X[ , 3:5]		# в xl координаты точек Petal.Length, Petal.Width и название класса , к которому относится соответствующая точка
  l <- dim(xl)[1] 
  n <- dim(xl)[2] - 1
  
  distances <- c()
  for(tmp in 1:l){
    distances[tmp] <- eDist(xl[tmp, 1:n], z)		# расстояния от точки z до каждой точки из выборки
  }
  orderedxl <- xl[order(distances), ]		# сортируем точки в порядке увеличения расстояния от z до точки из выборки
  classes <- orderedxl[1:k, n + 1]  		# заберем только первые k строк (получим k ближайших соседей), и в них оставим только столбец названий классов
  counts <- table(classes) 			# посчитаем, сколько раз встретился каждый класс
  return(names(which.max(counts)))		# точка z относится к тому классу, который чаще всего встречается в classes
}
```

Оптимальное *k* подбирается по *LOO* (скользящий контроль), который работает следующим образом: 

```
loo <- function(X, alg, step, x_max){
  Ox <- seq(from = 1, to = x_max, by = step)
  Oy <- c()
  l <- dim(X)[1]	
  LooOpt <- 1
  kOpt <- 1
  
  for(k in Ox){	#для разного числа соседей
    Q <- 0		# количество ошибок
    for(i in 1:l){
      X2 <- X[-i, ]	# из выборки X будем исключать по одной точке (z)
      z <- X[i, 3:4]	# запустим алгоритм(knn), как будто мы хотим классифицировать точку z, и имеем выборку X2.
      if(alg(z, X2, k) != X[i, 5]) Q <- Q + 1	# если алгоритм ошибся, увеличим количество ошибок Q
    }
    Loo <- Q/l 	# сумму ошибок делим на количество точек в выборке
    Oy <- c(Oy, Loo)
    if(Loo < LooOpt) {
      LooOpt <- Loo
      kOpt <- k	# оптимальным будет k, при котором величина Loo минимальна.
    }
  }
  return(kOpt)
}
```

Результат работы knn: 

![alt text](https://github.com/elena111111/R/blob/master/knn/knn.png)

Зависимость *Loo* от *k*:

![alt text](https://github.com/elena111111/R/blob/master/knn/knn_loo.png)


## Алгортим k взвешенных ближайших соседей (wknn).
Имеется выборка *X* (ирисы Фишера), и 3 класса (*setosa*, *versicolor*, *virginica*). 
Мы хотим классифицировать множество точек *{z}*.
Классификацию проводим по двум признакам (Petal.Length, Petal.Width), они же являются координатами точек.

*distances* - это вектор расстояний от точки *z* до каждой точки из выборки *X*.
*orderedxl* - это массив *xl*, отсортированный по возрастанию расстояний от точки *z* до каждой точки из выборки *X*.
Мы выбрали весовую функцию *q^i*.
Для *k* ближайших соседей построим соответствующие веса.
В массиве *classes* будет *k* строк, а в столбцах название класса и вес точки. 
Осталось найти сумму весов для каждого класса отдельно в *classes* (это массив *ans*).
Ответом будет тот класс, вес которого максимален в *ans*.

Значение *q* подбирается по *LOO*, который работает следующим образом:
Пусть переменная *Q = 0*.
Из выборки(*X*) будем исключать по одной точке (пусть будет точка *zi*) - получим выборку *X2*. 
Теперь запустим алгоритм(*wknn*), как будто мы хотим классифицировать точку *zi*, и имеем выборку *X2*.
Если алгоритм ошибся, то к величине *Q* прибавим 1.
Когда мы таким образом переберём все точки выборки, вычислим *Loo = Q/l*, где *l* - количество точек в выборке *X*.

Будем делать это для разных *q*, которые могут находиться в интервале *(0; 1)*.
Оптимальным будет *q*, при котором величина *Loo* минимальна.

Результат работы программы: 

![alt text](https://github.com/elena111111/R/blob/master/wknn/wknn.png)

Зависимость *Loo* от *q*:

![alt text](https://github.com/elena111111/R/blob/master/wknn/wknn_loo_q.png)

## Алгоритм парзеновского окна (pw).
Имеется выборка *X* (ирисы Фишера), и 3 класса (*setosa*, *versicolor*, *virginica*). 
Мы хотим классифицировать множество точек *{z}*.
Классификацию проводим по двум признакам (Petal.Length, Petal.Width), они же являются координатами точек.

*distances_weighed[ , 1]* - это расстояния от точки *z* до каждой точки из выборки *X*.
*distances_weighed[ , 2]* - значение весовой функции для каждой точки из выборки.
Весовая функция зависит от расстояния от точки *z* до точки из выборки, а также от выбора параметра *h* (ширина окна) и ядра K, которые подбираются по LOO (принцип работы уже был описан в прдыдущих алгоритмах).
Наглядно: мы строим вокруг точки *z*(центр окна) окрестность радиуса *h*, и смотрим, суммарный вес какого класса в этой окрестности больше.
Для каждого ядра получилось одинаковое значение минимального *Loo*. Так что мы убедились, что выбор ядра слабо влияет на классификацию.
В таблице *classes* хранятся расстояния от *z* до точек из *X*, веса точек из *X* и название класса каждой точки из *X*.
Суммируем веса каждого класса (матрица *ans*).
Ответом будет класс, для которого суммарный вес максимален в *ans*.

Стоит отметить, что данный алгоритм плохо подходит для неравномерно распределённой выборки (в окно одинаковой ширины попадает очень разное количество объектов). 
В этом случае лучше использовать алгоритм парзеновского окна с переменной шириной окна (описан ниже).

Результат работы программы / Зависимость *loo* от *h* (для разных ядер):

1) Прямоугольное ядро:

![alt text](https://github.com/elena111111/R/blob/master/pw/pw_core_rect_and_loo.png)

2) Треугольное ядро:

![alt text](https://github.com/elena111111/R/blob/master/pw/pw_core_triang_and_loo.png)

3) Ядро Епанечникова:

![alt text](https://github.com/elena111111/R/blob/master/pw/pw_core_epan_and_loo.png)

4) Квартическое ядро:

![alt text](https://github.com/elena111111/R/blob/master/pw/pw_core_quart_and_loo.png)

5) Гауссовское ядро: 

![alt text](https://github.com/elena111111/R/blob/master/pw/pw_core_gauss_and_loo.png)

## Алгоритм парзеновского окна с переменной шириной окна (varpw).
Имеется выборка *X* (ирисы Фишера), и 3 класса (*setosa*, *versicolor*, *virginica*). 
Мы хотим классифицировать множество точек *{z}*.
Классификацию проводим по двум признакам (Petal.Length, Petal.Width), они же являются координатами точек.
*distances* - это вектор расстояний от точки *z* до каждой точки из выборки *X*.
*orderedxl* - это массив *xl*, отсортированный по возрастанию расстояний от точки *z* до каждой точки из выборки *X*.
*distances_s* - отсортированный вектор расстояний.
*weights* - вектор весов для *k* ближайших соседей.
Для весовой функции нам нужно выбрать ядро(в примере работы программы используется прямоугольное).
Аргумент ядра - расстояние от *z* до *i*-го соседа точки *z* (*i* от *1* до *k*), деленое на расстояние от *z* до (*k+1*)-го соседа.

Возьмем из *orderedxl* первые *k* строк, допишем к ним соответствующие веса и оставим только столбцы названий классов и весов - получим массив *classes*.
Ответом будет тот класс, суммарный вес которого максимален в *classes*. 

Данный метод подходит для неравномерно распределенной выборки.

Пример работы программы:

![alt text](https://github.com/elena111111/R/blob/master/varpw/varpw_core_rect.png)

## Метод потенциальных функций 
Имеется выборка *X* (ирисы Фишера), и 3 класса (*setosa*, *versicolor*, *virginica*). 
Мы хотим классифицировать множество точек *{z}*.
Классификацию проводим по двум признакам (Petal.Length, Petal.Width), они же являются координатами точек.

В методе парзеновского окна мы помещали центр окна в классифицируемый объект, а теперь построим окрестности вокруг обучающих объектов. 
Причем каджая окрестность будет иметь свой потенциал - велину, показывающую, насколько сильно это окно влияет на другие элементы. То есть даже при одинаковой ширине окна *h* это влияние может быть разное.

Первым мы вызываем метод *gamma*, в теле которого происходит следующее: 
*g[dim(iris)[1]]* - это вектор потенциалов, изначально заполненный нулями. 

Вспомним:
Вспомогательный метод *loo* запускает алгоритм pf с текущими параметрами *g, h, K*(функция ядра) для выборки *X*, исключает из *X* по одной точке, 
тестирует алгоритм *pf* на оставшихся объектах и сравнивает результат работы алгоритма с эталонным результатом. Если ответы не совпали, то величину ошибок увеличиваем на 1. 
Суммарную величину ошибок делим на размерность выборки *X*. Метод вернет число от 0 до 1.
 
*eps* - это максимально допустимое число ошибок (от 0 до 1).
Если результат работы *loo > eps*, значит надо улучшить вектор *g*: 
Выберем случайным образом элемент *xi* из *X*.
Если алгортим *pf(X, xi, g, K, h)* ошибается, значит в точке *xi* нужно увеличить потенциал на 1.

Если *loo* с текущими пераметрами вернет число *<= eps*, значит мы достигли преемлемой доли ошибок, и *g* больше улучшать не будем. Вернем вектор *g*.

Шируну окна *h* можно подбирать индивидуально для каждого объекта, но в алгоритме не указывается, как именно (недостаток). 
В данном случае эта величина взята из метода парзеновского окна, и одинакова для всех окон.

Метод *pf* работает аналогично с методом парзеновского окна, только значение функции ядра ещё домножается на *g*:
*z* - классифицируемая точка.
*distances_weighed[ , 1]* - это расстояния от точки *z* до каждой точки из выборки *X*.
*distances_weighed[ , 2]* - значение весовой функции для каждой точки из выборки *X*.
*classes* - это *distances_weighed*, к которому добавили столбец соответствующих названий классов. 
Ответом для *z* будет тот класс, сумарный вес которого максимален в *classes*.

Пример работы программы(для гауссовского ядра):

![alt text](https://github.com/elena111111/R/blob/master/pf/pf_gauss_015_center.png)

Черным показаны центры окон с ненулевым потенциалом (он получился в них равен 1).

## Алгоритм STOLP
 



 
